{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data with Spark HATS _Pre_-_Exercises_\n",
    "\n",
    "These pre-exercises will introduce you to the basics of\n",
    "[Jupyter](https://jupyter.org) and verify that your environment\n",
    "is properly configured.\n",
    "\n",
    "*Note* - To perform any exercise, these notebooks must be open\n",
    "within [Jupyter](https://jupyter.org). GitHub has a very nice\n",
    "notebook renderer, but it is read-only and won't actually\n",
    "execute any code. Information on how to access Jupyter can\n",
    "be found in the [README](../README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Jupyter\n",
    "\n",
    "Developing via Jupyter is the inverse of how most people interact with Python. Normally, people write a whole script, pass it in bulk to Python, execute it, then examine the outputs.\n",
    "\n",
    "With Jupyter, however, you first begin by starting Python *then* you pass snippets of code for Python to execute. Since the code is added incrementally to a constantly-running python process, intermediate values stay in-memory, and the coding cycle of \"write, execute, print outputs, loop\" becomes much more interactive. This mode of development isn't totally new, REPL (Read, Execute, Print, Loop) shells exist for a number of languages.\n",
    "\n",
    "Below this text exists your first code entry box, known as a \"cell\" in Jupyter parlance. Enter the following code, then press Shift+Enter to tell Jupyter to execute it:\n",
    "```python\n",
    "x = 10\n",
    "y = 5\n",
    "print x + y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that after you press Shift+Enter, the text to the left of your cell changed to `In [1]`. Jupyter counts the number of times that cells are executed, and updates the legend for each cell. This is useful if you want to find the last cell you executed.\n",
    "\n",
    "This was a quick execution, so you may have missed it. While the Jupyter `kernel` is running, the cell's label will change to `In [*]`. While there's an asterisk, Python is running in the background. If it seems to be running too long, you can either interrupt the current execution by hitting `Kernel -> Interrupt` menu or abort the whole process via `Kernel -> Restart`.\n",
    "\n",
    "Once you run, you should get the expected value of `15`. Remember, though that the python process is still running in the background. You can verify this by printing out the value of `y` and note that it returns the expected value of `5`\n",
    "```python\n",
    "    print y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to press shift-enter to execute cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Spark is Properly Installed\n",
    "Each notebook runs as a separate Python process, but Spark itself is implemented in Scala (which is related to Java). The PySpark interface connects the two languages and handles passing data back and forth between the two.\n",
    "\n",
    "To gain access to Spark, you first need to create a SparkSession who loads the Scala binaries via PySpark. Once the connection is complete, PySpark returns a SparkSession, which is the central entrypoint to Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start up spark and get our SparkSession...\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(# Name of your application in the dashboard/UI\n",
    "             \"00-test-spark\"\n",
    "            ) \\\n",
    "    .config(# Tell Spark to load some extra libraries from Maven (the Java repository)\n",
    "            'spark.jars.packages',\n",
    "            'org.diana-hep:spark-root_2.11:0.1.16,org.diana-hep:histogrammar-sparksql_2.11:1.0.4'\n",
    "            ) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate π using Spark\n",
    "To verify that Spark is working, estimate π by randomly choosing one million points in the unit 2D plane and counting the number that fall within a unit circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "\n",
    "# SparkContexts provide most of the \"execution engine\" of Spark\n",
    "sc = spark.sparkContext\n",
    "# One million\n",
    "num_samples = 1000000\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4.0 * count / num_samples\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Spark-ROOT functionality\n",
    "When Spark was started, we loaded two additional Scala libraries into the environment:\n",
    "* **[Spark-ROOT](https://github.com/diana-hep/Spark-ROOT)** - Scala-based ROOT/IO interface to Spark\n",
    "* **[Histogrammar](https://histogrammar.org)** - Functional histogramming framework, optimized for Spark\n",
    "\n",
    "First, verify that Spark-ROOT is functional by loading a ROOT file and counting the number of events it contains. If successful, you will see this example file has four events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the ROOT file into a Spark DataFrame...\n",
    "import os\n",
    "testPath = \"file://%s/../root/test-tuple.root\" % os.getcwd()\n",
    "df = spark.read.format(\"org.dianahep.sparkroot.experimental\").load(testPath)\n",
    "# ... and print the number of events\n",
    "print df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Histogrammar works\n",
    "The second Scala library PySpark loaded was Histogrammar, which is used to quickly produce histograms of very large datasets in a distributed, functional manner. Verify it works by generating random numbers distributed according to the unit Gaussian distribution, adding them to a histogram, then plotting the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce 200,000 random numbers\n",
    "import random\n",
    "data = [random.gauss(0, 1) for i in range(200000)]\n",
    "\n",
    "# Define the histogram\n",
    "import histogrammar as hg\n",
    "gauss = hg.Bin(num=16,\n",
    "                   low=-4,\n",
    "                   high=4,\n",
    "                   quantity=lambda x: x,\n",
    "                   value=hg.Count())\n",
    "\n",
    "# Fill the histogram with the values we generated earlier\n",
    "for d in data:\n",
    "    gauss.fill(d)\n",
    "\n",
    "# Plot the resulting histogram\n",
    "print gauss.ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If successful, you will see a text representation of a unit Gaussian. This is underwhelming, so let's test the final piece of the puzzle, a plotting library.\n",
    "## Verify Matplotlib Functionality\n",
    "One of the very nice features of Jupyter is its ability to display visualisations in-line within the browser. To tell Jupyter to display images inline, we can use what is known as a\n",
    "[magic command](http://ipython.readthedocs.io/en/5.x/interactive/magics.html). Magic commands begin with a `%` character and are interpreted directly by Jupyter instead of being passed to Python. Tell jupyter by executing the following in the next cell\n",
    "```python\n",
    "    %matplotlib inline  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that Jupyter knows we want to see matplotlib outputs in the browser, plot the histogram we previously generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = gauss.plot.matplotlib(name=\"Unit Gaussian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete\n",
    "If each step ran without error, congratulations, your environment is properly set up!\n",
    "\n",
    "One note: Starting a notebook automatically starts a Python interpreter in the background, so you can close your web browser and come back to it later and resume where you left off. However, each Python process consumes resources, which can be significant if you're processing large datasets. To free up the resources, shut down the interpreter by clicking `File->Close and Halt` if you're done with a notebook. The server itself doesn't take up any resources, so you don't need to click the `Shutdown Server` button.\n",
    "\n",
    "If you're interested, feel free to browse the rest of the tutorial [index](../Start-Here.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
