{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Z-Mass peak with Spark\n",
    "With the basics of an analysis under our belt, let's look at di-muon events from 2016 CMS data.\n",
    "\n",
    "For sake of berevity, we'll skip a lot of steps that would be necessary for a \"real\" result. As we walk through this simplified analysis, note how and where these extra corrections could be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Like before, we need to begin our Spark session. You shouldn't need to change any of the following values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start up Spark (only needs to be done once)\n",
    "import os\n",
    "import pyspark.sql\n",
    "import sys\n",
    "session = pyspark.sql.SparkSession.builder \\\n",
    "    .appName(\"pyspark-zpeak\") \\\n",
    "    .config('spark.jars.packages', 'org.diana-hep:spark-root_2.11:0.1.16,org.diana-hep:histogrammar-sparksql_2.11:1.0.4') \\\n",
    "    .getOrCreate()\n",
    "sc = session.sparkContext\n",
    "sqlContext = session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ROOT into DataFrame\n",
    "The Spark-ROOT project lets us load ROOT files directly into dataframes. Execute the following to load a standard analysis-level ntuple into ROOT then print its schema. You should see familiar branches\n",
    "\n",
    "```python\n",
    "import os\n",
    "testPath = \"file://%s/../root/test-tuple.root\" % os.getcwd()\n",
    "df = spark.read \\\n",
    "            .format(\"org.dianahep.sparkroot\") \\\n",
    "            .load(testPath)\n",
    "df.printSchema()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Load the 2016 data\n",
    "The following snippet of code reads a list of input datasets from a JSON file and adds additional columns with the cross-sections and processing efficiencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "dataDFList = []\n",
    "mcDFList = []\n",
    "\n",
    "sys.stdout.write(\"\\nLoading DataFrames \")\n",
    "fileInputs = json.loads(open(\"input-hats.json\", \"r\").read())\n",
    "\n",
    "for category, subcategory, path, xsec, efficiency in fileInputs: \n",
    "    if category in (\"Category\", \"DISABLED\"):\n",
    "        continue\n",
    "    \n",
    "    dsPath = \"hdfs://cmshdfs/tmp/store/group/ra2tau/\" + path + \"/\"\n",
    "    tempDF = sqlContext.read.load(dsPath) \\\n",
    "                    .withColumn(\"Category\", lit(category)) \\\n",
    "                    .withColumn(\"SubCategory\", lit(subcategory)) \\\n",
    "                    .withColumn(\"xsec\", lit(xsec)) \\\n",
    "                    .withColumn(\"eff\", lit(efficiency))\n",
    "\n",
    "    if xsec == \"DATA\":\n",
    "        dataDFList.append([tempDF, category, subcategory, xsec, efficiency])\n",
    "    else:\n",
    "        mcDFList.append([tempDF, category, subcategory, xsec, efficiency])\n",
    "\n",
    "    sys.stdout.write('.')\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Combine DataFrames\n",
    "The previous step yields a DataFrame per-dataset, which can be unwieldy to manage. Spark's `union` function concatenates two DataFrames together. Let's loop over everything and make a DataFrame for MC and another for data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Monte-Carlo\n",
    "if mcDFList:\n",
    "    mcDF = mcDFList[0][0]\n",
    "    for x in mcDFList[1:]:\n",
    "        mcDF = mcDF.union(x[0])\n",
    "    print \"MC Partitions:  %d\" % mcDF.rdd.getNumPartitions()\n",
    "\n",
    "# Data\n",
    "if dataDFList:\n",
    "    dataDF = dataDFList[0][0]\n",
    "    for x in dataDFList[1:]:\n",
    "        dataDF = dataDF.union(x[0])\n",
    "    print \"Data Partitions: %d\" % dataDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Drop unneeded columns\n",
    "Spark is intelligent about how it executes queries, so any unneeded columns are simply not read (similar to how ROOT handles reading branches). That being said, these ntuples have 300 columns, so functions like `printSchema()` and `show()` are unintelligible. Execute the following cell to remove branches that won't be useful for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reqCol = ['Muon_pt', 'Muon_eta', 'Muon_phi', 'Muon_energy',\n",
    "          'Muon_charge', 'Muon_loose', 'Muon_tight', 'Muon_isGlobal',\n",
    "          'Jet_pt', 'Jet_eta', 'Jet_phi', 'Jet_energy',\n",
    "          'Met_type1PF_pt','Met_type1PF_px','Met_type1PF_py',\n",
    "          'Met_type1PF_pz','Met_type1PF_phi',\n",
    "          'eff', 'xsec', 'Category', 'SubCategory']\n",
    "trimMCDF = mcDF.select(*reqCol)\n",
    "trimDataDF = dataDF.select(*reqCol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Weigh our MC events\n",
    "Like usual, we'll need to weigh each MC event to take into the theoretical cross-section and processing efficiency. Let's add an additional column for the `xsec * efficiency * lumi` portion of that scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "processedLumiPB = 35900\n",
    "mcWeight = (trimMCDF['xsec'] * trimMCDF['eff'] * processedLumiPB)\n",
    "weightedMCDF = trimMCDF.withColumn(\"weight\", 1/mcWeight)\n",
    "weightedDataDF = trimDataDF.withColumn(\"weight\", lit(1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data weight?\n",
    "You probably noticed in the previous slide that we added a weight column for data as well.\n",
    "\n",
    "We would like to combine our data and MC DataFrames into a single DF, but DFs have the restriction that each row must have the same columns as every other row. In the next step, we'll combine the data and MC into one combined DF, so data also needs a weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Combine Data and MC frames\n",
    "trimDF = weightedMCDF.union(weightedDataDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Similar to doing a ROOT-based analysis, it is often helpful to split the transforms into two parts:\n",
    "\n",
    "1. A rarely changing part with either time consuming calculations or steps which drop many events\n",
    "2. A part under more active development\n",
    "\n",
    "Check-pointing after a strong cut (like the trigger) is relatively easy and is a huge boost to the time it takes to load the data for your analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transform our DataFrame\n",
    "At this point, we've loaded our data into Spark, and we're ready to begin applying object and event selections. We'll do these selections by applying a transformation to our input data to get a new DF. Then, we repeat the process by applying a transformation to this new DF to get another DF (and so on)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DataFrames are functional\n",
    "Producing results by chaining several idempotent transformations together on immutable objects is known as \"functional\" programming. There are several benefits to this technique:\n",
    "\n",
    "1. Easily-implemented lazy evaluation\n",
    "2. Easier to (auto-) parallelize\n",
    "3. \"Where\" an object came from is well-defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Our input\n",
    "Let's take a moment to look our input DF. First, let's look at the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "trimDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the next cell, let's look at the muon columns of our first row with a `select()` and a `show()`\n",
    "```python\n",
    "trimDF.select(\"Muon_pt\", \"Muon_eta\", \"Muon_phi\").show(1, truncate = False)\n",
    "```\n",
    "Spark implements this simple command by\n",
    "1. Executing `select()` against trimDF to produce a new (unnamed) DF\n",
    "2. Executing `show()` against this new DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Flatten our DataFrame\n",
    "As you can see from the previous cell (and `printSchema()`), some columns have lists and aren't plain scalar values.\n",
    "\n",
    "Since we're looking at di-muon events, let's replace the lists of muons with the properties of the two-highest pT muons in the event.\n",
    "\n",
    "This logic would be very hard to write as a `where()` statement. Let's use a User Defined Function (UDF) instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# User Defined Function (UDF)\n",
    "UDFs can be used for transformations that are more complex than can be expressed with `where()` and friends.\n",
    "\n",
    "For each row in the input DF, Spark will call your UDF and pass in the columns you request. The return value is added as a new column in the output DF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Unfortunately, Spark doesn't support having a single UDF fill multiple columns, so we have to cheat a bit and pack multiple values into a single column.\n",
    "\n",
    "First, let's define a custom type for our return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"pass\", BooleanType(), False),\n",
    "    StructField(\"mu1_pt\", FloatType(), False),\n",
    "    StructField(\"mu2_pt\", FloatType(), False),\n",
    "    StructField(\"mu1_eta\", FloatType(), False),\n",
    "    StructField(\"mu2_eta\", FloatType(), False),\n",
    "    StructField(\"mu1_phi\", FloatType(), False),\n",
    "    StructField(\"mu2_phi\", FloatType(), False),\n",
    "    StructField(\"mu1_charge\", FloatType(), False),\n",
    "    StructField(\"mu2_charge\", FloatType(), False),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then we can define the function that Spark will use. This function can be written in several languages, but let's use Python. It's quite a bit slower than the alternatives, but is simple for our tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Our `muonTestFalse()` function accepts lists of muon pt, eta, phi, charge and the \"loose\" ID flag.\n",
    "\n",
    "The logic in the loop is straightforward. Look through the muons lists to find exactly two candidates which match some properties.\n",
    "\n",
    "The return value has the properties of the selected muons with an additional flag called \"pass\", which is true if the muons pass our event-wide selections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "muonTestFalse = (False, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
    "def muonTest(pt,eta,phi,charge,loose):\n",
    "    \"\"\"\n",
    "    An absolutely unphysical dimuon selection function\n",
    "    \"\"\"\n",
    "    passes = True\n",
    "    if len(pt) < 2:\n",
    "        return muonTestFalse\n",
    "    \n",
    "    leadingIdx = None\n",
    "    trailingIdx = None\n",
    "    for idx in range(len(pt)):\n",
    "        if leadingIdx == None:\n",
    "            if pt[idx] > 30 and abs(eta[idx]) < 2.1 and loose[idx]:\n",
    "                leadingIdx = idx\n",
    "        elif trailingIdx == None:\n",
    "            if pt[idx] > 10 and abs(eta[idx]) < 2.1 and (loose[idx]):\n",
    "                trailingIdx = idx\n",
    "        else:\n",
    "            if pt[idx] > 10 and abs(eta[idx]) < 2.1 and loose[idx]:\n",
    "                return muonTestFalse\n",
    "    \n",
    "    if leadingIdx != None and trailingIdx != None:\n",
    "        return (True, pt[leadingIdx], pt[trailingIdx],\n",
    "                        eta[leadingIdx], eta[trailingIdx],\n",
    "                        phi[leadingIdx], phi[trailingIdx],\n",
    "                        charge[leadingIdx], charge[trailingIdx])\n",
    "    else:\n",
    "        return muonTestFalse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Some example muons to test we get the right thing from our UDF\n",
    "# This should pass\n",
    "print muonTest([52.707950592041016, 35.526939392089844],\n",
    "                 [0.24396079778671265, -0.08245399594306946],\n",
    "                 [-0.8336199522018433, 2.2777249813079834],\n",
    "                 [1.0, -1.0],\n",
    "                 [True, True])\n",
    "# This should fail\n",
    "print muonTest([52.707950592041016, 5.526939392089844],\n",
    "                 [0.24396079778671265, -0.08245399594306946],\n",
    "                 [-0.8336199522018433, 2.2777249813079834],\n",
    "                 [1.0, -1.0],\n",
    "                 [True, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Apply the UDF\n",
    "Now that we have the UDF and result type, let's tell Spark to produce an additional column with the results of the UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the python function to a Spark UDF\n",
    "from pyspark.sql.functions import udf\n",
    "muonUDF = udf(muonTest, schema)\n",
    "# Make a new column \"flat\" and fill it with the results of muonUDF\n",
    "flatDF = trimDF.withColumn(\"flat\", muonUDF(\"Muon_pt\", \n",
    "                                           \"Muon_eta\",\n",
    "                                           \"Muon_phi\",\n",
    "                                           \"Muon_charge\",\n",
    "                                           \"Muon_loose\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Use the cell below to inspect the new column we just made. Modify the `show()` statement we used a few cells before. It can be helpful to also pass `truncate = False` to `show()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Filter with event-level cuts\n",
    "The UDF interface allows us to fill a column, but it doesn't have the ability to throw away a row that failed certain conditions. Our UDF sets `flat.pass` to `False` for events that fail our cuts. We have to perform a second step to actually remove those events from the DataFrame.\n",
    "\n",
    "Make a new DF named `muonPassDF` with an additional requirement that `pass` is true. Then, `show()` the result.\n",
    "\n",
    "Tip: Since \"flat\" is a structure, the \"pass\" member is called flat.pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Caching\n",
    "By default, Spark will use some portion of its RAM to cache DFs. You can also instruct Spark to cache both in RAM and to disk with `cache()`. This is useful for DFs who have very expensive transformations you don't want to recalculate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicit caching can take quite a bit of time, so let's skip it for now\n",
    "# finalDF = muonPassDF.cache()\n",
    "\n",
    "# Instead, let's just pass the finalDF straight through\n",
    "finalDF = muonPassDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Your turn: Calculate the dimuon invariant mass\n",
    "Using the UDF example before, add a column to `finalDF` with the invariant mass of the dimuon system\n",
    "\n",
    "Tip: The relevant parts of ROOT's TLorentzVector is the following:\n",
    "```c\n",
    "void TLorentzVector::SetPtEtaPhiM(Double_t pt, Double_t eta, Double_t phi, Double_t m) {\n",
    "    pt = TMath::Abs(pt);\n",
    "    SetXYZM(pt*TMath::Cos(phi), pt*TMath::Sin(phi), pt*sinh(eta) ,m);\n",
    "}\n",
    "void TLorentzVector::SetXYZM(Double_t  x, Double_t  y, Double_t  z, Double_t m) {\n",
    "    if ( m  >= 0 )\n",
    "        SetXYZT( x, y, z, TMath::Sqrt(x*x+y*y+z*z+m*m) );\n",
    "    else\n",
    "        SetXYZT( x, y, z, TMath::Sqrt( TMath::Max((x*x+y*y+z*z-m*m), 0. ) ) );\n",
    "}\n",
    "inline void TLorentzVector::SetXYZT(Double_t  x, Double_t  y, Double_t  z, Double_t t) {\n",
    "    fP.SetXYZ(x, y, z);\n",
    "    SetT(t);\n",
    "}\n",
    "inline void TLorentzVector::SetT(Double_t a) {\n",
    "    fE = a;\n",
    "}\n",
    "TLorentzVector TLorentzVector::operator + (const TLorentzVector & q) const {\n",
    "    return TLorentzVector(fP+q.Vect(), fE+q.T());\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Finally: our requested plot\n",
    "At this point, our DF has the data we need. Run the following snippet in the next cell to load Matplotlib and Histogrammar\n",
    "\n",
    "```python\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import histogrammar as hg\n",
    "import histogrammar.sparksql\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Your turn: Plot the dimuon invariant mass\n",
    "Using what you learned in the first part, plot the dimuon invariant mass for the different Monte-Carlo processes. Don't worry about making the plots pretty, that's another tutorial!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# More to try\n",
    "If you have time, try the following:\n",
    "* Manually implement \"isolation\" by requiring a dR between jets and muons\n",
    "* Plot additional kinematic variables (not just the inv. mass)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hats-spark",
   "language": "python",
   "name": "hats-spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
