{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Building Blocks\n",
    "## The important pieces to analyze (a lot of) data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RDD - Resiliant Distributed Dataset\n",
    "Spark's basic data collection is an RDD:\n",
    "* *Resiliant* - Any piece that is lost can be regenerated\n",
    "* *Distributed* - Pieces are scattered across multiple nodes\n",
    "* *Dataset* - A large number of \"items\" either generated from files or by translating other RDDs\n",
    "\n",
    "No restriction that each \"item\" in an RDD have the same elements, somewhat low-level for many uses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DataFrame\n",
    "DataFrames are specialized RDDs that organize their data in rows and columns, where every row has the same columns. This is similar to CMS datasets where each event in Data/MC has the same branches.\n",
    "\n",
    "The layout of a DataFrame is more limited than a plain RDD, but this limitation allows Spark's query optimizer to speed up or even elide portions of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outline\n",
    "The tools are different, but the overall workflow is familiar:\n",
    "\n",
    "1. Import datasets from disk\n",
    "2. Apply transformations\n",
    "  * Perform cuts\n",
    "  * Produce derived values\n",
    "3. Aggregate and report to produce tables/plots\n",
    "\n",
    "Let's first demonstrate the tools and concepts with a small, non-CMS dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For our first example, we will use the US Government's records of all flights in the US during 2015 (found [here](http://stat-computing.org/dataexpo/)). It's a small dataset that Spark is ridiculously overpowered for, but its size lets us experiment instantaneously. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setup\n",
    "Before we do anything with Spark, we must create a `SparkSession` like you did in the pre-exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "spark = SparkSession.builder \\\n",
    "            .appName(\"10-airline-data\") \\\n",
    "            .config(\"spark.jars.packages\", \"org.diana-hep:histogrammar-sparksql_2.11:1.0.4\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvPath = \"hdfs://cmshdfs/tmp/2005.csv.bz2\"\n",
    "airline = spark.read \\\n",
    "            .option(\"header\",\"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .csv(csvPath) \\\n",
    "            .sort(\"Month\", \"DayofMonth\") \\\n",
    "            .withColumnRenamed(\"UniqueCarrier\", \"Carrier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline = airline.withColumn(\"DepTime\", airline[\"DepTime\"].cast(IntegerType()))\\\n",
    "                    .withColumn(\"ArrTime\", airline[\"ArrTime\"].cast(IntegerType()))\\\n",
    "                    .withColumn(\"DepDelay\", airline[\"DepDelay\"].cast(IntegerType()))\\\n",
    "                    .withColumn(\"ArrDelay\", airline[\"ArrDelay\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html?highlight=cache#pyspark.sql.DataFrame.drop\n",
    "airline = airline.drop(\"CarrierDelay\", \"WeatherDelay\", \"NASDelay\", \"SecurityDelay\")\n",
    "airline.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmedAirline = airline.select(\"Month\", \"DayOfMonth\", \"DayOfWeek\",\n",
    "                                \"DepTime\", \"ArrDelay\", \"DepDelay\",\n",
    "                                \"TaxiIn\", \"TaxiOut\", \"Origin\",\n",
    "                                \"Dest\", \"Distance\", \"Carrier\",\n",
    "                                \"FlightNum\", \"Cancelled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmedAirline.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some filtering\n",
    "trimmedAirline.where(trimmedAirline.Carrier != \"UA\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about a more complicated expression?\n",
    "```python\n",
    "trimmedAirline.where(trimmedAirline.Month == 1 and trimmedAirline.DayOfMonth == 8).show(5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uh oh..\n",
    "```\n",
    "ValueError: Cannot convert column into bool: please use '&' for 'and'\n",
    "```\n",
    "Let's try that....\n",
    "```python\n",
    "trimmedAirline.where(trimmedAirline.Month == 1 & trimmedAirline.DayOfMonth == 8).show(5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous cell complains:\n",
    "```\n",
    "Method and([class java.lang.Integer]) does not exist\n",
    "```\n",
    "The \"&\" operator has weird operator precidence, so the previous line:\n",
    "```python\n",
    "trimmedAirline.where(trimmedAirline.Month == 1 & trimmedAirline.DayOfMonth == 8).show(5)\n",
    "```\n",
    "Is interpreted as\n",
    "```python\n",
    "trimmedAirline.where(trimmedAirline.Month == (1 & trimmedAirline.DayOfMonth) == 8).show(5)\n",
    "```\n",
    "Add explicit parenthesis to force what we really mean:\n",
    "```python\n",
    "trimmedAirline.where((trimmedAirline.Month == 1) & (trimmedAirline.DayOfMonth == 8)).show(5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"SQL string\" format\n",
    "This is a lot easier for complicated expressions\n",
    "```python\n",
    "trimmedAirline.where(\"Month == 1 and DayOfMonth == 8\").show(5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmedAirline.where(\"Month == 1 and Origin == 'ORD' and Dest == 'EWR'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmedAirline.where(\"Month == 1 and DayOfMonth == 9 and Origin == 'ORD' and Dest == 'EWR'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmedAirline.where(\"\"\"Month == 1 AND DayOfMonth == 9 AND \n",
    "                        ((Origin == 'ORD' AND Dest == 'EWR') OR\n",
    "                         (Origin == 'EWR' AND Dest == 'ORD'))\"\"\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed = trimmedAirline.withColumnRenamed(\"DayOfMonth\", \"Day\")\n",
    "renamed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce derived values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a simple column for \"How much time was 'made up in the air'\"\n",
    "makeupTime = renamed.withColumn(\"Makeup\", (renamed.ArrDelay - renamed.DepDelay).cast(\"integer\"))\n",
    "makeupTime.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we want something thats more complicated than just simple arithmetic?\n",
    "# We can make a UDF - User Defined Function\n",
    "\n",
    "# Spark will execute the following function for each row. You can put arbitrary python\n",
    "# code here (with some restrictions)\n",
    "import datetime\n",
    "def dayOfYear(month, day):\n",
    "    \"\"\"\n",
    "    Given a month and day, return the day of year.\n",
    "        (e.g. Jan 1st is day \"0\")\n",
    "    \"\"\"\n",
    "    return datetime.date(2005,month,day).timetuple()[7] - 1\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "dayUDF = udf(dayOfYear, IntegerType())\n",
    "\n",
    "withDay = makeupTime.withColumn(\"DayCount\", dayUDF(\"Month\", \"Day\"))\n",
    "withDay.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you wanted to correct Jet energies and make corresponding changes\n",
    "# to the MET of the event. It would be difficult and inefficient to run two UDFs.\n",
    "# Fortunately, we aren't limited to a single output column. We can pass back a\n",
    "# structure to spark, and it will expand each memeber into a column\n",
    "\n",
    "dateSchema = StructType([\n",
    "    StructField(\"Date\", DateType(), False),\n",
    "    StructField(\"Count\", IntegerType(), False),\n",
    "])\n",
    "\n",
    "def dateUDF(month, day):\n",
    "    \"\"\"\n",
    "    Given a month and day, return the day of year and the \n",
    "        (e.g. Jan 1st is day \"0\")\n",
    "    \"\"\"\n",
    "    dateObj = datetime.date(2005,month,day)\n",
    "    dayCount = dateObj.timetuple()[7] - 1\n",
    "    return (dateObj, dayCount)\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "dayUDF = udf(dateUDF, dateSchema)\n",
    "\n",
    "withDayNested = makeupTime.withColumn(\"udf2\", dayUDF(\"Month\", \"Day\"))\n",
    "withDayNested.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "withDayFlat = withDayNested.select(\"udf2.Date\", \"udf2.Count\")\n",
    "withDayFlat.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = withDayNested \\\n",
    "                .withColumn(\"FlatDate\", withDayNested.udf2.Date) \\\n",
    "                .withColumn(\"FlatCount\", withDayNested.udf2.Count) \\\n",
    "                .drop(withDayNested.udf2)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Aggregate and Report\n",
    "Once we've loaded our data, done some filters and added some generated values, we'll want to aggregate the information so we can report values and/or make plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# How many flights are cancelled on each day of the week\n",
    "days = [\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"]\n",
    "cancelledPerDay = []\n",
    "for i in range(len(days)):\n",
    "    cancelledPerDay.append(data.where(\"DayOfWeek == %i and Cancelled == 1\" % (i + 1)).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in zip(days, cancelledPerDay):\n",
    "    print \"%s: %i\" % i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Load matplotlib and histogrammar\n",
    "For simple queries, a combination of `select` and `count` are sufficient, but more advanced queries need additional packages. Run the following snippet in the next cell to load Matplotlib and Histogrammar\n",
    "\n",
    "```python\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import histogrammar as hg\n",
    "import histogrammar.sparksql\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decorate our DataFrame\n",
    "With Histogrammar loaded, we can make our DataFrame \"Histogrammar-aware\" by executing the following:\n",
    "    \n",
    "```python\n",
    "    hg.sparksql.addMethods(data)\n",
    "```\n",
    "\n",
    "As you can probably guess, `addMethods` adds some magic methods to allow our Dataframe to be processed by Histogrammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg.sparksql.addMethods(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Plot a single variable\n",
    "At this point, we've got all the piece we need to actually extract some meaningful information from our dataset. Let's run the simplest-possible plotting in the next cell with:\n",
    "\n",
    "```python\n",
    "%%timeit -n3\n",
    "h1 = data.Bin(100, 0, 200, data['ArrDelay'])\n",
    "ax = h1.plot.matplotlib(name=\"Arrival Delay (min)\")\n",
    "```\n",
    "\n",
    "The `%%timeit` magic command will execute the cell 3 times and report the best iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What just happened?\n",
    "When the previous cell was executed, Spark did the following using several distributed workers:\n",
    "1. Load the data from the CSV and divide into a number of `partitions`\n",
    "2. Perform each transformation on the partitions\n",
    "3. Histogrammar fills a histogram per-partition\n",
    "4. Histogrammar uses Spark to perform a distributed reduce/aggregation\n",
    "5. The resulting histogram is passed to Matplotlib to be plotted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why is this useful?\n",
    "* Spark hides a lot of the complexity\n",
    "    * Splitting tasks\n",
    "    * Retrying failed jobs\n",
    "* Spark can optimize away unwanted computation\n",
    "* Spark can aggressively cache intermediate results\n",
    "* Distributed reduces are fast!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# More examples\n",
    "Now that we've plotted a single variable, let's do some more complicated aggregations and plots.\n",
    "\n",
    "Let's make a histogram of the departure delay, following the template from before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2 = data.Bin(100, 0, 200, data['DepDelay'])\n",
    "ax = h2.plot.matplotlib(name=\"Departure Delay (min)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What we've effectively done in the two previous cells is make two histograms (`h1` and `h2`), then filled them independently. In the background, Spark performed two separate aggregations. Histogrammar instead lets us combine the histograms together and perform a single aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note only the outer UntypedLabel connects to data. The\n",
    "# internal Bin objects connect to histogrammar. The outer\n",
    "# container will pass the elements in\n",
    "h3 = data.UntypedLabel(arrdelay=hg.Bin(100, 0, 200, data['ArrDelay']),\n",
    "                       depdelay=hg.Bin(100, 0, 200, data['DepDelay']),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = h3.get('depdelay').plot.matplotlib(name=\"Departure Delay (min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = h3.get('arrdelay').plot.matplotlib(name=\"Arrival Delay (min)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple plots\n",
    "It can be helpful to show multiple plots in a single cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "dd = h3.get('depdelay').plot.matplotlib(name=\"Departure Delay (min)\")\n",
    "plt.subplot(1, 2, 2)\n",
    "ad = h3.get('arrdelay').plot.matplotlib(name=\"Arrival Delay (min)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2D histograms\n",
    "Implicit in the `Bin()` definition we've seen so far is the value we want to fill. By default, the value is filled by `Count()`, which effectively adds a one into the appropriate bin.\n",
    "```python\n",
    "h4 = data.Bin(100, 0, 200, data['DepDelay'], value=hg.Count())\n",
    "```\n",
    "We can make a 2D histogram by replacing `Count()` with a 1D histogram\n",
    "```python\n",
    "h4 = data.Bin(100, 0, 200, data['DepDelay'],\n",
    "              value=hg.Bin(100, 0, 200, data['ArrDelay']))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Selecting a subset of events\n",
    "The `Select()` function lets us extract a subset of events\n",
    "```python\n",
    "h5 = data.UntypedLabel(\n",
    "        ua=hg.Select(data['Carrier'] == \"UA\", \n",
    "                         hg.Bin(100, 0, 200, data['ArrDelay'])),\n",
    "        aa=hg.Select(data['Carrier'] == \"AA\", \n",
    "                         hg.Bin(100, 0, 200, data['ArrDelay']))\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting two histograms simultaneously\n",
    "```python\n",
    "plt.xlabel('Minutes Delayed')\n",
    "h5.get('ua').plot.matplotlib(alpha=0.4, label='United')\n",
    "h5.get('aa').plot.matplotlib(alpha=0.4, label='American')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Delays by airline')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other aggregation operators\n",
    "Histogrammar provides a [number](http://histogrammar.org/python/1.0.9/) of additional aggregation operators. For instance, we can get the average arrival delay like the following.\n",
    "```python\n",
    "h6 = data.UntypedLabel(\n",
    "        ua=hg.Select(data['Carrier'] == \"UA\", \n",
    "                         hg.Average(data['ArrDelay'])),\n",
    "        aa=hg.Select(data['Carrier'] == \"AA\", \n",
    "                         hg.Average(data['ArrDelay']))\n",
    "    )\n",
    "print h6.get('ua').cut.mean\n",
    "```                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Your turn!\n",
    "Before we move to a CMS dataset, use what you've learned so far to count the number of cancelled flights for each month. Do you notice anything odd?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Other ideas\n",
    "If you've got extra time, answer the following:\n",
    "1. What are the 10 most popular routes?\n",
    "2. What airports are the most delayed outbound? Inbound?\n",
    "3. Show the trend of \"mins made up\" vs. distance.\n",
    "4. What airline has the lowest average delay?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "We looked all US flights in 2005 and studied their on-time arrival statistics. We used Spark, Histogrammar, and Matplotlib to:\n",
    "1. Import datasets from disk\n",
    "2. Apply transformations\n",
    "  * Perform cuts\n",
    "  * Produce derived values\n",
    "3. Aggregate and report to produce tables/plots\n",
    "\n",
    "Next, we'll apply these lessons to CMS data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Don't forget to clean up!\n",
    "Once you've completed this section and you're satisfied with the results, select \"File->Close and Halt\" from the Jupyter window. Spark is memory intensive, so it's good to remember to close the notebook when you're done. (Closing the notebook terminates the Spark process)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "hats-spark",
   "language": "python",
   "name": "hats-spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
